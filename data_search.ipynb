{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dotenv.load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/Final_TestSet/Final_TestSet.json', 'r', encoding='utf-8') as f:\n",
    "    dataset_init=json.load(f)\n",
    "with open('data/Final_Example.json', 'r', encoding='utf-8') as f:\n",
    "    preliminary_example=json.load(f)\n",
    "\n",
    "for i in range(0, len(dataset_init)):\n",
    "    # 检查数据集文件是否一致\n",
    "    assert dataset_init[i][\"ID\"] == preliminary_example[i][\"ID\"] \n",
    "    assert dataset_init[i][\"question\"] == preliminary_example[i][\"question\"]\n",
    "    \n",
    "print(\"样本数量：\",len(dataset_init))\n",
    "# print(\"问题类型：\",\",\".join(set([item[\"problem_type\"] for item in dataset_init])))\n",
    "\n",
    "\n",
    "FROM=0\n",
    "TO=FROM+100\n",
    "dataset=dataset_init[FROM:TO]"
   ],
   "id": "ffdd5e309a334511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gpt4o=ChatOpenAI(\n",
    "    api_key=os.getenv(\"WLAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"WLAI_BASE_URL\"),\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "gpt4o.invoke(\"hello\")\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\")) # "
   ],
   "id": "ed9aa0c611d6cdc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 预处理",
   "id": "574afcb7b791980c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 修改文件位置\n",
    "1. 构建prompt\n",
    "2. 修改题目中文件名位置"
   ],
   "id": "785a6b9588f98da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gpt4o import *\n",
    "for i in range(0, len(dataset)):\n",
    "    content = d_template[dataset[i][\"problem_type\"]].format(dataset[i][\"question\"])\n",
    "    filenames = extract_filenames(content)\n",
    "    for filename in filenames:\n",
    "        content = content.replace(filename, add_path(filename, data_path / 'Final_TestSet/data'))\n",
    "    dataset[i][\"content\"]=content"
   ],
   "id": "60042329e696a5cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### rag添加文档信息\n",
    "1.  翻译\n",
    "2. 从翻译提取函数和库\n",
    "3. 查询\n",
    "\n",
    "如何衡量：暂时不做\n",
    "\n",
    "\n"
   ],
   "id": "bba5e17b099af03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tool.model import translate_prompt\n",
    "\n",
    "# 翻译所有问题，已经缓存，所以全量翻译\n",
    "translation_runnable= translate_prompt | gpt4o | StrOutputParser()\n",
    "translation_list = translation_runnable.batch([{\"text\":item[\"question\"]} for item in dataset], config={\"max_concurrency\":1}, return_exceptions=True)\n",
    "for i in range(0,len(dataset)):\n",
    "    dataset[i][\"translation\"]=translation_list[i]"
   ],
   "id": "645074f81eea53b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tool.model import extract_runnable\n",
    "\n",
    "# 从翻译中提取出函数和库\n",
    "extract_list=extract_runnable.batch([{\"text\":item[\"translation\"]} for item in dataset[:]], config={\"max_concurrency\":5}, return_exceptions=True)\n",
    "for i in range(len(extract_list)):\n",
    "    # print(i+1,extract_list[i])\n",
    "    dataset[i][\"func_extract\"]=extract_list[i]"
   ],
   "id": "5eb6aaef2040ba7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tool.rag_tool import search_documents_by_help_function\n",
    "\n",
    "for i,key_work in tqdm(enumerate(extract_list), total=len(extract_list)):\n",
    "    key_work=key_work if type(key_work) is list else [key_work]\n",
    "    tmp_list=[]\n",
    "    for kw in key_work:\n",
    "        doc=search_documents_by_help_function(kw[\"function_name\"],kw[\"module_name\"])\n",
    "        tmp_list.append(\"<api doc>\\n\"+doc+\"\\n</api doc>\")\n",
    "    dataset[i][\"rag_infos\"]=tmp_list\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i][\"content\"]=dataset[i][\"content\"]+\"\\n\\n\"+\"\\n\".join(dataset[i][\"rag_infos\"])"
   ],
   "id": "913bd5026dc2b63d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 添加目标",
   "id": "ade469fcd36fab0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from tool.model import cal_prompt, draw_prompt, tof_prompt\n",
    "\n",
    "def get_goals(text:str, problem_type:str):\n",
    "    types=[]\n",
    "    goals=[]\n",
    "    if problem_type.startswith(\"multi\"):\n",
    "        types.extend(problem_type[6:-1].split(\", \"))\n",
    "    else:\n",
    "        types.append(problem_type)\n",
    "\n",
    "    for t in types:\n",
    "        if t==\"calculations\":\n",
    "            prompt=cal_prompt\n",
    "        elif t==\"True/False\":\n",
    "            prompt=tof_prompt\n",
    "        elif t==\"draw\":\n",
    "            prompt=draw_prompt\n",
    "        else:\n",
    "            raise Exception(\"unknown problem type\")\n",
    "    \n",
    "        runnable=prompt|gpt4o|StrOutputParser()\n",
    "        goal=runnable.invoke({\"question\":text})\n",
    "        goals.append(goal)\n",
    "    return goals\n",
    "        \n",
    "        \n",
    "for i in tqdm(range(len(dataset))):\n",
    "    if dataset[i][\"problem_type\"].startswith(\"multi\"):\n",
    "        goals=get_goals(dataset[i][\"question\"], dataset[i][\"problem_type\"])\n",
    "        # print(i+1,goals)\n",
    "        dataset[i][\"goals\"]=goals\n",
    "        dataset[i][\"content\"]=dataset[i][\"content\"]+\"\\n\\n\"+\"\\nwe need to answer following question：\\n\"+\"\\n\".join(goals)\n",
    "    "
   ],
   "id": "2dc0dfd1db5c315b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 运行\n",
    "### 运行agent"
   ],
   "id": "a0ca324b051de187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from autogen import Cache\n",
    "\n",
    "def run(item: dict,cache_seed=2):\n",
    "    content = item[\"content\"]\n",
    "    item[\"content\"]=content\n",
    "\n",
    "    # Use DiskCache as cache\n",
    "    with Cache.disk(cache_path_root=\"./autogen_cache\",cache_seed=cache_seed) as cache:\n",
    "        chat_result = code_executor_agent.initiate_chat(\n",
    "            code_writer_agent,\n",
    "            message=content,\n",
    "            summary_method='reflection_with_llm',\n",
    "            summary_args=dict(summary_prompt='only return the code output'),\n",
    "            cache=cache,\n",
    "            # silent=True,\n",
    "        )\n",
    "    # code = extract_python_code(chat_result.chat_history[-3]['content'])[-1]\n",
    "    code=\"\"\n",
    "    for i in range(len(chat_result.chat_history)-1, 0, -1):\n",
    "        l=extract_python_code(chat_result.chat_history[i]['content'])\n",
    "        if len(l)>0:\n",
    "            code=l[-1]\n",
    "            break\n",
    "    \n",
    "    answer = chat_result.summary\n",
    "    if isinstance(answer, dict):\n",
    "        answer = answer['content']\n",
    "    item[\"code\"]=code\n",
    "    item[\"answer\"]=answer\n",
    "    # item['chat_history']=chat_result.chat_history\n",
    "    return item\n",
    "\n",
    "for item in tqdm(dataset[92:93]):\n",
    "    run(item)\n"
   ],
   "id": "372043cc6b22dc2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### voting",
   "id": "5ecaccc4a0c04f1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for item in [dataset[i-1] for i in [3,6,9,10,12,14,19,21,23,24,28,30,37,40]]:\n",
    "#     temp_answer=[]\n",
    "#     for seed in tqdm(range(1,17)):\n",
    "#         item=run(item,seed)\n",
    "#         code,answer=item[\"code\"],item[\"answer\"]\n",
    "#         temp_answer.append(answer)\n",
    "#     prompt=f\"\"\"从下面的不同人表达中，直接返回大部分人想表达的内容，不附带其他信息：\\n\"\"\"+\"\\n\".join(temp_answer)\n",
    "#     print(item[\"ID\"],(gpt4o|StrOutputParser()).invoke(prompt))\n",
    "        \n",
    "        "
   ],
   "id": "4c573789ee14ab78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 存储",
   "id": "573bc4288f92333d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d415250e3495dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/SMP_240913_check_1.json', 'w', encoding='utf-8') as f:\n",
    "    s = json.dumps(dataset, indent=4, ensure_ascii=False)\n",
    "    f.write(s)"
   ],
   "id": "99d59b27c6efe78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "28657a61d400979b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "raise Exception(\"stop\")",
   "id": "4f8860b7e6239bd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/SMP_240905_check_1.json', 'r', encoding='utf-8') as f:\n",
    "    tmp_dataset=json.load(f)"
   ],
   "id": "5148f3e17df16072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(dataset)):\n",
    "    print(dataset[i][\"problem_type\"])"
   ],
   "id": "d940da249dd28e7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tmp_id=50\n",
    "i=tmp_id-1\n",
    "print(tmp_dataset[i][\"ID\"], tmp_dataset[i][\"problem_type\"],\"\\n---\\n\", tmp_dataset[i][\"translation\"],\"\\n---\\n\", tmp_dataset[i]['answer'],\"\\n---\\n\",tmp_dataset[i][\"code\"],\"\\n---\\n\",tmp_dataset[i][\"question\"])"
   ],
   "id": "5dc053e492298f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tool.rag_tool import search_documents\n",
    "\n",
    "def remove_empty_values(d):\n",
    "    \"\"\"\n",
    "    递归删除字典中的所有空值（包括空字符串、空列表、空字典、None等）\n",
    "    \"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "    \n",
    "    # 使用字典推导式递归遍历字典\n",
    "    return {k: remove_empty_values(v) for k, v in d.items() if v not in ('', None, [], {}, set(), ())}\n",
    "\n",
    "\n",
    "for i,key_work in tqdm(enumerate(extract_list), total=len(extract_list)):\n",
    "    infos=\"\"\n",
    "    INFO_LIMIT=3000\n",
    "\n",
    "    for item in key_work:\n",
    "        if item[\"function_name\"] != \"\":\n",
    "            module,function = item['module_name'],item['function_name']\n",
    "            api_docs=search_documents(function,module,dataset[i][\"question\"])\n",
    "            for doc in api_docs[:2]:\n",
    "                if len(infos)<INFO_LIMIT:\n",
    "                    if not doc.startswith(\"no\"):\n",
    "                        doc=json.dumps(remove_empty_values(json.loads(doc)))\n",
    "                    infos=infos + \"\\n\\n\"+doc\n",
    "        # 没有抽取，尝试用整个问题查询\n",
    "        else:\n",
    "            api_docs=search_documents(method_description=dataset[i][\"question\"])\n",
    "            for doc in api_docs:\n",
    "                if len(infos)<INFO_LIMIT:\n",
    "                    if not doc.startswith(\"no\"):\n",
    "                        doc=json.dumps(remove_empty_values(json.loads(doc)))\n",
    "                    infos=infos + \"\\n\\n\"+doc\n",
    "    dataset[i][\"rag_infos\"]=infos\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i][\"content\"]=dataset[i][\"content\"]+\"\\n\\n\"+dataset[i][\"rag_infos\"]"
   ],
   "id": "fd8a2ceb820f7a5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(dataset)):\n",
    "    rag_infos=dataset[i][\"rag_infos\"]\n",
    "    print()\n",
    "    print(i+1,extract_list[i],[round(t[1]+t[2],2)  for t in rag_infos])\n",
    "        "
   ],
   "id": "64541ba1e7004493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tool.rag_tool import search_documents_by_help_function\n",
    "\n",
    "for i in range(0,len(dataset)):\n",
    "    for item in extract_list[i]:\n",
    "        fn=item[\"function_name\"].split(\".\")[-1]\n",
    "        mo=str(item[\"module_name\"]).lower().strip().split(\".\")[0]\n",
    "        print(i, fn, mo,end=\" \")\n",
    "        doc=search_documents_by_help_function(fn, mo)\n",
    "        if doc:\n",
    "            print(len(doc))\n",
    "        else:\n",
    "            print(None)\n",
    "            "
   ],
   "id": "146431f170989d4e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
